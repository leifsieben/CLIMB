# ==============================================================================
# Pretraining Pipeline Configuration Example
# ==============================================================================

name: combined_pretrain_run
output_dir: ./experiments/pretrain_pipeline_example
tokenizer_path: ./tokenizer

# ---------------------------------------------------------------------------
# Model
# ---------------------------------------------------------------------------
model:
  hidden_size: 512
  num_hidden_layers: 8
  num_attention_heads: 8
  intermediate_size: 2048
  max_position_embeddings: 512
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1

# ---------------------------------------------------------------------------
# Compute Budget
# ---------------------------------------------------------------------------
compute_budget:
  total_epochs: 20
  supervised_fraction: 0.5

# ---------------------------------------------------------------------------
# MLM Training (using pre-tokenized SMILES)
# ---------------------------------------------------------------------------
mlm_training:
  batch_size: 64
  learning_rate: 5e-5
  warmup_steps: 1000
  weight_decay: 0.01
  mlm_probability: 0.15
  logging_steps: 100
  save_steps: 1000
  eval_steps: 0
  fp16: false
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  save_total_limit: 2
  dataloader_num_workers: 0
  evaluation_strategy: no

unsupervised_data:
  - data/unsup.pkl

# ---------------------------------------------------------------------------
# Supervised Tasks
# ---------------------------------------------------------------------------
tasks:
  - name: BBBP
    task_type: binary_classification
    metric: roc_auc
    description: Blood-brain barrier penetration

  - name: ESOL
    task_type: regression
    metric: rmse
    description: Aqueous solubility (log mol/L)

data_sources:
  - path: data/bbbp.csv
    SMILES_column: SMILES
    label_mapping:
      p_np: BBBP

  - path: data/esol.csv
    SMILES_column: SMILES
    label_mapping:
      measured_log_solubility: ESOL

supervised_training:
  batch_size: 32
  learning_rate: 2.0e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  logging_steps: 100
  save_steps: 1000
  eval_steps: 500
  fp16: false
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0

validation_fraction: 0.1
