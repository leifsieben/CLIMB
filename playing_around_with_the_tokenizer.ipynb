{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debe7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c54faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading SMILES data...\n",
      "✓ Loaded 8346066 SMILES strings\n",
      "\n",
      "Training tokenizer with vocab_size=1000, min_frequency=2...\n",
      "Training in progress...\n",
      "\n",
      "\n",
      "\n",
      "✓ Training complete!\n",
      "\n",
      "============================================================\n",
      "✓ Tokenizer trained and saved to local_prototyping_data/tokenizer\n",
      "✓ Vocabulary size: 1000\n",
      "✓ Special tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Setup paths\n",
    "data_dir = Path(\"./local_prototyping_data\")\n",
    "unsupervised_file = data_dir / \"GDB17_subset.smi\"\n",
    "\n",
    "# Read the CSV (just one column of SMILES)\n",
    "print(\"Reading SMILES data...\")\n",
    "df = pd.read_csv(unsupervised_file, header=None, names=[\"SMILES\"])\n",
    "print(f\"✓ Loaded {len(df)} SMILES strings\")\n",
    "\n",
    "# Extract SMILES as list\n",
    "smiles_list = df[\"SMILES\"].tolist()\n",
    "\n",
    "# Configuration\n",
    "VOCAB_SIZE = 1000\n",
    "MIN_FREQUENCY = 2\n",
    "TOKENIZER_DIR = data_dir / \"tokenizer\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(f\"\\nTraining tokenizer with vocab_size={VOCAB_SIZE}, min_frequency={MIN_FREQUENCY}...\")\n",
    "tokenizer_trainer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "# Train on the clean file\n",
    "print(\"Training in progress...\")\n",
    "tokenizer_trainer.train(\n",
    "    files=[str(unsupervised_file)],\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=MIN_FREQUENCY,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "\n",
    "print(\"✓ Training complete!\")\n",
    "\n",
    "# Save tokenizer\n",
    "TOKENIZER_DIR.mkdir(exist_ok=True, parents=True)\n",
    "tokenizer_trainer.save(str(TOKENIZER_DIR / \"tokenizer.json\"))\n",
    "\n",
    "# Convert to HuggingFace format\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=str(TOKENIZER_DIR / \"tokenizer.json\"),\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "\n",
    "# Save HF tokenizer\n",
    "tokenizer.save_pretrained(str(TOKENIZER_DIR))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Tokenizer trained and saved to {TOKENIZER_DIR}\")\n",
    "print(f\"✓ Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"✓ Special tokens: {tokenizer.all_special_tokens}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03ed395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing tokenizer on sample SMILES from your data:\n",
      "\n",
      "  SMILES: BrC1=C2C3=C4C(CC3CCC2=O)C(=N)NC4=N1\n",
      "  Tokens: ['BrC', '1', '=', 'C', '2', 'C', '3', '=', 'C', '4', 'C', '(', 'CC', '3', 'CCC', '2', '=', 'O', ')', 'C', '(=', 'N', ')', 'NC', '4', '=', 'N', '1']\n",
      "  Num tokens: 28\n",
      "  Token IDs (first 10): [310, 21, 33, 39, 22, 39, 23, 33, 39, 24]\n",
      "\n",
      "  SMILES: BrC1=C2C3C4CCC(C4)C3C(=N)OC2=NC=C1\n",
      "  Tokens: ['BrC', '1', '=', 'C', '2', 'C', '3', 'C', '4', 'CCC', '(', 'C', '4', ')', 'C', '3', 'C', '(=', 'N', ')', 'OC', '2', '=', 'NC', '=', 'C', '1']\n",
      "  Num tokens: 27\n",
      "  Token IDs (first 10): [310, 21, 33, 39, 22, 39, 23, 39, 24, 262]\n",
      "\n",
      "  SMILES: BrC1=C2C3C4CCC(O4)C3(OC2=NC=C1)C#C\n",
      "  Tokens: ['BrC', '1', '=', 'C', '2', 'C', '3', 'C', '4', 'CCC', '(', 'O', '4', ')', 'C', '3', '(', 'OC', '2', '=', 'NC', '=', 'C', '1', ')', 'C', '#', 'C']\n",
      "  Num tokens: 28\n",
      "  Token IDs (first 10): [310, 21, 33, 39, 22, 39, 23, 39, 24, 262]\n",
      "\n",
      "  SMILES: BrC1=C2C3C4CNC(C4)(C#N)C3OC2=NC=C1\n",
      "  Tokens: ['BrC', '1', '=', 'C', '2', 'C', '3', 'C', '4', 'CNC', '(', 'C', '4', ')(', 'C', '#', 'N', ')', 'C', '3', 'OC', '2', '=', 'NC', '=', 'C', '1']\n",
      "  Num tokens: 27\n",
      "  Token IDs (first 10): [310, 21, 33, 39, 22, 39, 23, 39, 24, 276]\n",
      "\n",
      "  SMILES: BrC1=C2C3=C4C(OC(=O)C4=CC2=O)=CC3=NO1\n",
      "  Tokens: ['BrC', '1', '=', 'C', '2', 'C', '3', '=', 'C', '4', 'C', '(', 'OC', '(=', 'O', ')', 'C', '4', '=', 'CC', '2', '=', 'O', ')=', 'CC', '3', '=', 'NO', '1']\n",
      "  Num tokens: 29\n",
      "  Token IDs (first 10): [310, 21, 33, 39, 22, 39, 23, 33, 39, 24]\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer on your actual SMILES\n",
    "print(\"\\nTesting tokenizer on sample SMILES from your data:\")\n",
    "test_samples = smiles_list[:5]\n",
    "for smiles in test_samples:\n",
    "    tokens = tokenizer.tokenize(smiles)\n",
    "    ids = tokenizer.encode(smiles)\n",
    "    print(f\"\\n  SMILES: {smiles}\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Num tokens: {len(tokens)}\")\n",
    "    print(f\"  Token IDs (first 10): {ids[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af53fee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 4: Testing Tokenizer\n",
      "================================================================================\n",
      "\n",
      "Tokenization examples:\n",
      "\n",
      "  SMILES: CCO\n",
      "  Tokens: ['CCO']\n",
      "  IDs: [289]\n",
      "  Decoded: CCO\n",
      "\n",
      "  SMILES: c1ccccc1\n",
      "  Tokens: ['c', '1', 'ccccc', '1']\n",
      "  IDs: [71, 21, 655, 21]\n",
      "  Decoded: c1ccccc1\n",
      "\n",
      "  SMILES: CC(=O)O\n",
      "  Tokens: ['CC', '(=', 'O', ')', 'O']\n",
      "  IDs: [261, 265, 51, 13, 51]\n",
      "  Decoded: CC(=O)O\n",
      "\n",
      "\n",
      "Special token IDs:\n",
      "  PAD: <pad> -> 1\n",
      "  MASK: <mask> -> 4\n",
      "  BOS: <s> -> 0\n",
      "  EOS: </s> -> 2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 4: Testing Tokenizer\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test tokenization\n",
    "test_smiles = [\"CCO\", \"c1ccccc1\", \"CC(=O)O\"]\n",
    "\n",
    "print(\"\\nTokenization examples:\")\n",
    "for smiles in test_smiles:\n",
    "    tokens = tokenizer.tokenize(smiles)\n",
    "    ids = tokenizer.encode(smiles)\n",
    "    decoded = tokenizer.decode(ids)\n",
    "    \n",
    "    print(f\"\\n  SMILES: {smiles}\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  IDs: {ids[:10]}...\" if len(ids) > 10 else f\"  IDs: {ids}\")\n",
    "    print(f\"  Decoded: {decoded}\")\n",
    "\n",
    "# Test special tokens\n",
    "print(\"\\n\\nSpecial token IDs:\")\n",
    "print(f\"  PAD: {tokenizer.pad_token} -> {tokenizer.pad_token_id}\")\n",
    "print(f\"  MASK: {tokenizer.mask_token} -> {tokenizer.mask_token_id}\")\n",
    "print(f\"  BOS: {tokenizer.bos_token} -> {tokenizer.bos_token_id}\")\n",
    "print(f\"  EOS: {tokenizer.eos_token} -> {tokenizer.eos_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144dcca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 5: Tokenizing Unsupervised Dataset\n",
      "================================================================================\n",
      "\n",
      "Tokenizing 8346066 unsupervised samples...\n",
      "✓ Tokenized 8346066 samples\n",
      "\n",
      "Example tokenized sample:\n",
      "  Input IDs shape: 28 tokens\n",
      "  Input IDs: [310, 21, 33, 39, 22, 39, 23, 33, 39, 24, 39, 12, 261, 23, 262, 22, 33, 51, 13, 39, 265, 50, 13, 264, 24, 33, 50, 21]\n",
      "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Sequence length statistics:\n",
      "  Min: 3\n",
      "  Max: 38\n",
      "  Mean: 22.44\n",
      "  Median: 23.00\n",
      "\n",
      "✓ Saved tokenized data to local_prototyping_data/unsupervised_tokenized.pkl\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 5: Tokenizing Unsupervised Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def tokenize_unsupervised(smiles_list: List[str], max_length: int = 512):\n",
    "    \"\"\"Tokenize SMILES for unsupervised learning\"\"\"\n",
    "    tokenized = []\n",
    "    \n",
    "    for smiles in smiles_list:\n",
    "        encoding = tokenizer(\n",
    "            smiles,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False,  # We'll pad in batches\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        tokenized.append({\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "        })\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Use smiles_list instead of sample_smiles_unsupervised\n",
    "print(f\"\\nTokenizing {len(smiles_list)} unsupervised samples...\")\n",
    "unsup_tokenized = tokenize_unsupervised(smiles_list, MAX_LENGTH)\n",
    "\n",
    "print(f\"✓ Tokenized {len(unsup_tokenized)} samples\")\n",
    "print(f\"\\nExample tokenized sample:\")\n",
    "print(f\"  Input IDs shape: {len(unsup_tokenized[0]['input_ids'])} tokens\")\n",
    "print(f\"  Input IDs: {unsup_tokenized[0]['input_ids']}\")\n",
    "print(f\"  Attention mask: {unsup_tokenized[0]['attention_mask']}\")\n",
    "\n",
    "# Statistics\n",
    "seq_lengths = [len(item['input_ids']) for item in unsup_tokenized]\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(f\"  Min: {min(seq_lengths)}\")\n",
    "print(f\"  Max: {max(seq_lengths)}\")\n",
    "print(f\"  Mean: {np.mean(seq_lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(seq_lengths):.2f}\")\n",
    "\n",
    "# Save tokenized data\n",
    "unsup_tokenized_file = data_dir / \"unsupervised_tokenized.pkl\"\n",
    "with open(unsup_tokenized_file, 'wb') as f:\n",
    "    pickle.dump(unsup_tokenized, f)\n",
    "print(f\"\\n✓ Saved tokenized data to {unsup_tokenized_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77114061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 6: Loading and Tokenizing Supervised Dataset\n",
      "================================================================================\n",
      "Loaded supervised dataset:\n",
      "  Shape: (11622, 984)\n",
      "  Columns: ['full_id', 'pert_id', 'cell_iname', 'SMILES', 'inchi_key', 'compound_aliases', 'geneID-10007', 'geneID-1001', 'geneID-10013', 'geneID-10038']...\n",
      "\n",
      "✓ Found 978 gene expression columns\n",
      "  SMILES samples: 11622\n",
      "  Labels shape: (11622, 978)\n",
      "  Example label (first compound, first 5 genes): [2 2 2 2 2]\n",
      "\n",
      "Tokenizing 11622 supervised samples...\n",
      "✓ Tokenized 11622 samples with labels\n",
      "\n",
      "Example tokenized supervised sample:\n",
      "  SMILES: CN(C)[N+][O-]\n",
      "  Input IDs: [268, 12, 39, 324, 50, 316, 63, 51, 317]...\n",
      "  Label shape: (978,)\n",
      "  Label (first 5 genes): [2 2 2 2 2]\n",
      "\n",
      "✓ Saved tokenized supervised data to local_prototyping_data/supervised_tokenized.pkl\n",
      "\n",
      "================================================================================\n",
      "SECTION 7: Testing Data Loading\n",
      "================================================================================\n",
      "\n",
      "Loading unsupervised data...\n",
      "✓ Loaded 8346066 unsupervised samples\n",
      "\n",
      "Loading supervised data...\n",
      "✓ Loaded 11622 supervised samples\n",
      "✓ Loaded 11622 labels\n",
      "✓ Label dimensionality: (11622, 978)\n",
      "✓ Gene columns saved: 978\n",
      "\n",
      "Verifying data integrity...\n",
      "✓ All data verified successfully!\n",
      "\n",
      "============================================================\n",
      "Summary:\n",
      "  Unsupervised samples: 8346066\n",
      "  Supervised samples: 11622\n",
      "  Genes per sample: 978\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 6: Loading and Tokenizing Supervised Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load supervised data\n",
    "sup_data_file = data_dir / \"LINCS_L1000_MCF7_0-4.csv\"  # Replace with your actual filename\n",
    "df_sup = pd.read_csv(sup_data_file)\n",
    "\n",
    "print(f\"Loaded supervised dataset:\")\n",
    "print(f\"  Shape: {df_sup.shape}\")\n",
    "print(f\"  Columns: {df_sup.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
    "\n",
    "# Extract SMILES column\n",
    "smiles_supervised = df_sup['SMILES'].tolist()\n",
    "\n",
    "# Extract all gene expression columns (all columns starting with 'geneID-')\n",
    "gene_columns = [col for col in df_sup.columns if col.startswith('geneID-')]\n",
    "print(f\"\\n✓ Found {len(gene_columns)} gene expression columns\")\n",
    "\n",
    "# Create labels matrix (each row is all gene expressions for one compound)\n",
    "labels_supervised = df_sup[gene_columns].values  # This is a numpy array of shape (n_samples, n_genes)\n",
    "\n",
    "print(f\"  SMILES samples: {len(smiles_supervised)}\")\n",
    "print(f\"  Labels shape: {labels_supervised.shape}\")\n",
    "print(f\"  Example label (first compound, first 5 genes): {labels_supervised[0][:5]}\")\n",
    "\n",
    "# Tokenize supervised data\n",
    "def tokenize_supervised(smiles_list: List[str], labels: np.ndarray, max_length: int = 512):\n",
    "    \"\"\"Tokenize SMILES for supervised learning\"\"\"\n",
    "    tokenized = []\n",
    "    \n",
    "    for smiles in smiles_list:\n",
    "        encoding = tokenizer(\n",
    "            smiles,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        tokenized.append({\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "        })\n",
    "    \n",
    "    return tokenized, labels\n",
    "\n",
    "print(f\"\\nTokenizing {len(smiles_supervised)} supervised samples...\")\n",
    "sup_tokenized, sup_labels = tokenize_supervised(\n",
    "    smiles_supervised, \n",
    "    labels_supervised, \n",
    "    MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"✓ Tokenized {len(sup_tokenized)} samples with labels\")\n",
    "print(f\"\\nExample tokenized supervised sample:\")\n",
    "print(f\"  SMILES: {smiles_supervised[0]}\")\n",
    "print(f\"  Input IDs: {sup_tokenized[0]['input_ids'][:20]}...\")\n",
    "print(f\"  Label shape: {sup_labels[0].shape}\")\n",
    "print(f\"  Label (first 5 genes): {sup_labels[0][:5]}\")\n",
    "\n",
    "# Save tokenized supervised data\n",
    "sup_tokenized_file = data_dir / \"supervised_tokenized.pkl\"\n",
    "with open(sup_tokenized_file, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'data': sup_tokenized, \n",
    "        'labels': sup_labels,\n",
    "        'gene_columns': gene_columns  # Save gene names for reference\n",
    "    }, f)\n",
    "print(f\"\\n✓ Saved tokenized supervised data to {sup_tokenized_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: Test Loading Tokenized Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 7: Testing Data Loading\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load unsupervised data\n",
    "print(\"\\nLoading unsupervised data...\")\n",
    "with open(unsup_tokenized_file, 'rb') as f:\n",
    "    loaded_unsup = pickle.load(f)\n",
    "print(f\"✓ Loaded {len(loaded_unsup)} unsupervised samples\")\n",
    "\n",
    "# Load supervised data\n",
    "print(\"\\nLoading supervised data...\")\n",
    "with open(sup_tokenized_file, 'rb') as f:\n",
    "    loaded_sup = pickle.load(f)\n",
    "print(f\"✓ Loaded {len(loaded_sup['data'])} supervised samples\")\n",
    "print(f\"✓ Loaded {len(loaded_sup['labels'])} labels\")\n",
    "print(f\"✓ Label dimensionality: {loaded_sup['labels'].shape}\")\n",
    "print(f\"✓ Gene columns saved: {len(loaded_sup['gene_columns'])}\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"\\nVerifying data integrity...\")\n",
    "assert len(loaded_unsup) == len(unsup_tokenized), \"Unsupervised data mismatch!\"\n",
    "assert len(loaded_sup['data']) == len(sup_tokenized), \"Supervised data mismatch!\"\n",
    "assert loaded_unsup[0]['input_ids'] == unsup_tokenized[0]['input_ids'], \"Data corrupted!\"\n",
    "print(\"✓ All data verified successfully!\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Summary:\")\n",
    "print(f\"  Unsupervised samples: {len(loaded_unsup)}\")\n",
    "print(f\"  Supervised samples: {len(loaded_sup['data'])}\")\n",
    "print(f\"  Genes per sample: {loaded_sup['labels'].shape[1]}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a93098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 10: Creating PyTorch Datasets\n",
      "================================================================================\n",
      "✓ Created unsupervised dataset with 8346066 samples\n",
      "✓ Created supervised dataset with 11622 samples\n",
      "\n",
      "Testing dataset access:\n",
      "  Unsupervised sample 0 keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "  Supervised sample 0 keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "  Supervised sample 0 keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "  Supervised sample 0 label shape: torch.Size([978])\n",
      "  Supervised sample 0 label (first 10 genes): [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 10: Creating PyTorch Datasets\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class UnsupervisedChemicalDataset(Dataset):\n",
    "    \"\"\"Dataset for unsupervised learning\"\"\"\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class SupervisedChemicalDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised learning\"\"\"\n",
    "    def __init__(self, tokenized_data, labels):\n",
    "        self.data = tokenized_data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx].copy()\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "unsup_dataset = UnsupervisedChemicalDataset(loaded_unsup)\n",
    "sup_dataset = SupervisedChemicalDataset(loaded_sup['data'], loaded_sup['labels'])\n",
    "\n",
    "print(f\"✓ Created unsupervised dataset with {len(unsup_dataset)} samples\")\n",
    "print(f\"✓ Created supervised dataset with {len(sup_dataset)} samples\")\n",
    "\n",
    "# Test dataset access\n",
    "print(\"\\nTesting dataset access:\")\n",
    "print(f\"  Unsupervised sample 0 keys: {unsup_dataset[0].keys()}\")\n",
    "print(f\"  Supervised sample 0 keys: {sup_dataset[0].keys()}\")\n",
    "print(f\"  Supervised sample 0 keys: {sup_dataset[0].keys()}\")\n",
    "print(f\"  Supervised sample 0 label shape: {sup_dataset[0]['labels'].shape}\")\n",
    "print(f\"  Supervised sample 0 label (first 10 genes): {sup_dataset[0]['labels'][:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9baeba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 9: Testing Data Collator for Batching\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting a batch from dataloader...\n",
      "✓ Batch created successfully!\n",
      "\n",
      "Batch contents:\n",
      "  input_ids shape: torch.Size([4, 28])\n",
      "  attention_mask shape: torch.Size([4, 28])\n",
      "  labels shape: torch.Size([4, 28])\n",
      "\n",
      "Example of masking (first sample):\n",
      "  Original tokens: [310, 644, 33, 39, 22, 39, 23, 33, 39, 24, 39, 12, 261, 23, 4, 22, 33, 51, 13, 39, 4, 50, 13, 264, 24, 33, 50, 21]...\n",
      "  Masked positions: [1, 14, 20]...\n",
      "  Mask token ID: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 9: Testing Data Collator for Batching\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Create data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,  # 15% of tokens will be masked\n",
    ")\n",
    "\n",
    "# Create a small dataloader\n",
    "dataloader = DataLoader(\n",
    "    unsup_dataset,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Get one batch\n",
    "print(\"\\nGetting a batch from dataloader...\")\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(f\"✓ Batch created successfully!\")\n",
    "print(f\"\\nBatch contents:\")\n",
    "print(f\"  input_ids shape: {batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "print(f\"  labels shape: {batch['labels'].shape}\")\n",
    "\n",
    "print(f\"\\nExample of masking (first sample):\")\n",
    "original_ids = batch['input_ids'][0]\n",
    "labels = batch['labels'][0]\n",
    "masked_positions = (labels != -100).nonzero(as_tuple=True)[0]\n",
    "print(f\"  Original tokens: {original_ids.tolist()[:30]}...\")\n",
    "print(f\"  Masked positions: {masked_positions.tolist()[:10]}...\")\n",
    "print(f\"  Mask token ID: {tokenizer.mask_token_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78104587",
   "metadata": {},
   "source": [
    "# How long does context window need to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edeeeb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Length Analysis:\n",
      "================================================================================\n",
      "\n",
      "SMILES: CC[C@@H]1[C@@]([C@@H]([C@H](C(=O)[C@@H](C[C@@]([C@@H]([C@H](...\n",
      "Characters: 180\n",
      "Tokens: 163\n",
      "Tokens: ['CC', '[', 'C', '@', '@', 'H', ']', '1', '[', 'C', '@', '@', ']', '([', 'C', '@', '@', 'H', ']', '([']...\n",
      "\n",
      "SMILES: NC(=O)C[C@@]8(C)[C@H](CCC(N)=O)C=2/N=C8/C(/C)=C1/[C@@H](CCC(...\n",
      "Characters: 254\n",
      "Tokens: 209\n",
      "Tokens: ['NC', '(=', 'O', ')', 'C', '[', 'C', '@', '@', ']', '8', '(', 'C', ')[', 'C', '@', 'H', ']', '(', 'CCC']...\n",
      "\n",
      "SMILES: CC1=C2[C@@]([C@]([C@H]([C@@H]3[C@]4([C@H](OC4)C[C@@H]([C@]3(...\n",
      "Characters: 167\n",
      "Tokens: 132\n",
      "Tokens: ['CC', '1', '=', 'C', '2', '[', 'C', '@', '@', ']', '([', 'C', '@', ']', '([', 'C', '@', 'H', ']', '([']...\n",
      "\n",
      "================================================================================\n",
      "YOUR DATA ANALYSIS:\n",
      "================================================================================\n",
      "Total molecules: 8346066\n",
      "Min length: 3\n",
      "Max length: 38\n",
      "Mean: 22.4\n",
      "Median: 23\n",
      "95th percentile: 29\n",
      "99th percentile: 31\n",
      "99.9th percentile: 33\n",
      "\n",
      "Truncation analysis:\n",
      "  max_len=64: 0 truncated (0.00%)\n",
      "  max_len=128: 0 truncated (0.00%)\n",
      "  max_len=256: 0 truncated (0.00%)\n",
      "  max_len=512: 0 truncated (0.00%)\n",
      "  max_len=1024: 0 truncated (0.00%)\n",
      "\n",
      "Longest molecule (index 212149):\n",
      "  Length: 38 tokens\n",
      "  SMILES: CC1C2=C3C4=C1N=C1N(C)C5=C(N41)C3=C(O5)S2...\n"
     ]
    }
   ],
   "source": [
    "erythromycin = \"CC[C@@H]1[C@@]([C@@H]([C@H](C(=O)[C@@H](C[C@@]([C@@H]([C@H]([C@@H]([C@H](C(=O)O1)C)O[C@H]2C[C@@]([C@H]([C@@H](O2)C)O)(C)OC)C)O[C@H]3[C@@H]([C@H](C[C@H](O3)C)N(C)C)O)(C)O)C)C)O)(C)O\"\n",
    "vitamin_b12 = \"NC(=O)C[C@@]8(C)[C@H](CCC(N)=O)C=2/N=C8/C(/C)=C1/[C@@H](CCC(N)=O)[C@](C)(CC(N)=O)[C@@](C)(N1[Co+]C#N)[C@@H]7/N=C(C(\\C)=C3/N=C(/C=2)C(C)(C)[C@@H]3CCC(N)=O)[C@](C)(CCC(=O)NCC(C)OP([O-])(=O)O[C@@H]6[C@@H](CO)O[C@H](n5cnc4cc(C)c(C)cc45)[C@@H]6O)[C@H]7CC(N)=O\"\n",
    "paclitaxol = \"CC1=C2[C@@]([C@]([C@H]([C@@H]3[C@]4([C@H](OC4)C[C@@H]([C@]3(C(=O)[C@@H]2OC(=O)C)C)O)OC(=O)C)OC(=O)c5ccccc5)(C[C@@H]1OC(=O)[C@H](O)[C@@H](NC(=O)c6ccccc6)c7ccccc7)O)(C)C\"\n",
    "\n",
    "test_smiles = [erythromycin, vitamin_b12, paclitaxol]\n",
    "\n",
    "# Tokenize and check lengths\n",
    "print(\"Token Length Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "for smiles in test_smiles:\n",
    "    tokens = tokenizer.tokenize(smiles)\n",
    "    token_ids = tokenizer.encode(smiles)\n",
    "    print(f\"\\nSMILES: {smiles[:60]}...\" if len(smiles) > 60 else f\"\\nSMILES: {smiles}\")\n",
    "    print(f\"Characters: {len(smiles)}\")\n",
    "    print(f\"Tokens: {len(token_ids)}\")\n",
    "    print(f\"Tokens: {tokens[:20]}...\" if len(tokens) > 20 else f\"Tokens: {tokens}\")\n",
    "\n",
    "# Now check YOUR actual data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"YOUR DATA ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all lengths from your tokenized data\n",
    "all_lengths = [len(item['input_ids']) for item in unsup_tokenized]\n",
    "\n",
    "import numpy as np\n",
    "print(f\"Total molecules: {len(all_lengths)}\")\n",
    "print(f\"Min length: {min(all_lengths)}\")\n",
    "print(f\"Max length: {max(all_lengths)}\")\n",
    "print(f\"Mean: {np.mean(all_lengths):.1f}\")\n",
    "print(f\"Median: {np.median(all_lengths):.0f}\")\n",
    "print(f\"95th percentile: {np.percentile(all_lengths, 95):.0f}\")\n",
    "print(f\"99th percentile: {np.percentile(all_lengths, 99):.0f}\")\n",
    "print(f\"99.9th percentile: {np.percentile(all_lengths, 99.9):.0f}\")\n",
    "\n",
    "print(\"\\nTruncation analysis:\")\n",
    "for max_len in [64, 128, 256, 512, 1024]:\n",
    "    n_truncated = sum(1 for l in all_lengths if l > max_len)\n",
    "    pct = n_truncated / len(all_lengths) * 100\n",
    "    print(f\"  max_len={max_len}: {n_truncated} truncated ({pct:.2f}%)\")\n",
    "\n",
    "# Find the longest SMILES\n",
    "longest_idx = all_lengths.index(max(all_lengths))\n",
    "print(f\"\\nLongest molecule (index {longest_idx}):\")\n",
    "print(f\"  Length: {all_lengths[longest_idx]} tokens\")\n",
    "print(f\"  SMILES: {smiles_list[longest_idx][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd766545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climb (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
